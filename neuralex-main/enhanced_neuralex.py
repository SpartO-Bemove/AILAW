"""
–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è neuralex —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
"""
import logging
import logging
import time
import threading
from typing import List, Optional
from langchain.schema import Document
from langchain_community.vectorstores import Chroma
from neuralex_main import neuralex
from document_loader import DocumentLoader

logger = logging.getLogger(__name__)

class EnhancedNeuralex(neuralex):
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è neuralex —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    
    def __init__(self, llm, embeddings, vector_store, redis_url=None, documents_path="documents"):
        super().__init__(llm, embeddings, vector_store, redis_url)
        
        self.document_loader = DocumentLoader(documents_path)
        self.additional_documents_loaded = False
        self.documents_stats = {}
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        self._load_additional_documents()
    
    def _load_additional_documents(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É"""
        try:
            logger.info("üîÑ –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã
            if self._should_skip_loading():
                logger.info("‚ö° –î–æ–∫—É–º–µ–Ω—Ç—ã —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é")
                self.additional_documents_loaded = True
                self.documents_stats = self.document_loader.get_documents_stats()
                return
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
            additional_docs = self.document_loader.load_all_documents()
            
            if additional_docs:
                logger.info(f"üìö –ù–∞–π–¥–µ–Ω–æ {len(additional_docs)} –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É
                self._add_documents_to_vector_store(additional_docs)
                self.additional_documents_loaded = True
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                self.documents_stats = self.document_loader.get_documents_stats()
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∫—É –æ –∑–∞–≥—Ä—É–∑–∫–µ
                self._save_loading_marker()
                
                logger.info("‚úÖ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
            else:
                logger.info("üìù –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                self.documents_stats = self.document_loader.get_documents_stats()
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
            # –ù–µ –ø—Ä–µ—Ä—ã–≤–∞–µ–º —Ä–∞–±–æ—Ç—É, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å –±–∞–∑–æ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é
    
    def _should_skip_loading(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –∑–∞–≥—Ä—É–∑–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        try:
            import os
            import json
            
            marker_file = "documents/.loaded_marker"
            if not os.path.exists(marker_file):
                return False
            
            # –ß–∏—Ç–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ—Å–ª–µ–¥–Ω–µ–π –∑–∞–≥—Ä—É–∑–∫–µ
            with open(marker_file, 'r') as f:
                marker_data = json.load(f)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∏–∑–º–µ–Ω–∏–ª–∏—Å—å –ª–∏ —Ñ–∞–π–ª—ã
            current_stats = self.document_loader.get_documents_stats()
            if marker_data.get('stats') != current_stats:
                return False
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞
            if not self.vector_store or not hasattr(self.vector_store, '_collection'):
                return False
            
            return True
            
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –º–∞—Ä–∫–µ—Ä–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
            return False
    
    def _save_loading_marker(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–∞—Ä–∫–µ—Ä –æ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–µ"""
        try:
            import os
            import json
            from datetime import datetime
            
            marker_data = {
                'loaded_at': datetime.now().isoformat(),
                'stats': self.documents_stats
            }
            
            os.makedirs("documents", exist_ok=True)
            with open("documents/.loaded_marker", 'w') as f:
                json.dump(marker_data, f, indent=2)
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –º–∞—Ä–∫–µ—Ä–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
    
    def _add_documents_to_vector_store(self, documents: List[Document]):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É"""
        try:
            if not documents:
                return
            
            # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –±–∞—Ç—á–∞–º–∏ –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            batch_size = 50
            for i in range(0, len(documents), batch_size):
                batch = documents[i:i + batch_size]
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                texts = [doc.page_content for doc in batch]
                metadatas = [doc.metadata for doc in batch]
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ Chroma
                self.vector_store.add_texts(texts=texts, metadatas=metadatas)
                
                logger.debug(f"–î–æ–±–∞–≤–ª–µ–Ω –±–∞—Ç—á {i//batch_size + 1}: {len(batch)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è
            if hasattr(self.vector_store, 'persist'):
                self.vector_store.persist()
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É: {e}")
            raise
    
    def reload_documents(self):
        """–ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã"""
        try:
            logger.info("üîÑ –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
            self._load_additional_documents()
            return True
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
            return False
    
    def get_documents_info(self) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö"""
        info = {
            'additional_documents_loaded': self.additional_documents_loaded,
            'stats': self.documents_stats,
            'base_vector_store_available': self.vector_store is not None
        }
        return info
    
    def conversational(self, query, session_id):
        """
        –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
        """
        try:
            # –í—ã–∑—ã–≤–∞–µ–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–π –º–µ—Ç–æ–¥
            return super().conversational(query, session_id)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ conversational –¥–ª—è session {session_id}: {e}")
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏, 
            # –ø—Ä–æ–±—É–µ–º —Ä–∞–±–æ—Ç–∞—Ç—å —Ç–æ–ª—å–∫–æ —Å –±–∞–∑–æ–≤–æ–π –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–æ–π
            if self.additional_documents_loaded:
                logger.info("–ü—Ä–æ–±—É–µ–º –æ—Ç–≤–µ—Ç–∏—Ç—å –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤—É—é –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É...")
                try:
                    # –í—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
                    self.additional_documents_loaded = False
                    result = super().conversational(query, session_id)
                    self.additional_documents_loaded = True  # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º
                    return result
                except Exception as e2:
                    logger.error(f"–û—à–∏–±–∫–∞ –∏ —Å –±–∞–∑–æ–≤–æ–π –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–æ–π: {e2}")
            
            # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –ø–æ–º–æ–≥–ª–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º fallback –æ—Ç–≤–µ—Ç
            fallback_answer = (
                "‚ùå –ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.\n\n"
                "–í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:\n"
                "‚Ä¢ –ü—Ä–æ–±–ª–µ–º—ã —Å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ–º –∫ OpenAI API\n"
                "‚Ä¢ –í—Ä–µ–º–µ–Ω–Ω—ã–µ –Ω–µ–ø–æ–ª–∞–¥–∫–∏ —Å –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö\n"
                "‚Ä¢ –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤\n\n"
                "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –µ—â–µ —Ä–∞–∑ —á–µ—Ä–µ–∑ –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç."
            )
            
            return fallback_answer, []